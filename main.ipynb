{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Preprocessing Utils](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(statement):\n",
    "    # Filter out stop words & special characters\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    special_characters = '''!()-â€”[]{};:'\"\\, <>./?@#$%^&*_~+='''\n",
    "    tokens = word_tokenize(statement)\n",
    "    return [token.lower() for token in tokens if token.lower() not in stop_words and token not in special_characters]\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    wordnet_tags = {\"V\": wordnet.VERB, \"R\": wordnet.ADV,\"N\": wordnet.NOUN,\"J\": wordnet.ADJ} \n",
    "    # Get parts of speech tag & determine the class in wordnet\n",
    "    pos_tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    pos_tag_class = wordnet_tags.get(pos_tag, wordnet.NOUN)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Lemmaitze with Part of Speech Tag to get the pure word\n",
    "    lemma = lemmatizer.lemmatize(word, pos=pos_tag_class)\n",
    "    return lemma\n",
    "\n",
    "def lemmatize_stmt(statement):    \n",
    "    filtered_tokens = tokenizer(statement)\n",
    "    lemmatize_tokens = []\n",
    "    for word in filtered_tokens:\n",
    "        lemma = lemmatize_word(word)\n",
    "        lemmatize_tokens.append(lemma)\n",
    "    return ' '.join(lemmatize_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Search Engine Practical](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Document Collection](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\acer/nltk_data'\n    - 'c:\\\\Project\\\\Environment\\\\env\\\\nltk_data'\n    - 'c:\\\\Project\\\\Environment\\\\env\\\\share\\\\nltk_data'\n    - 'c:\\\\Project\\\\Environment\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\acer\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Project\\Environment\\env\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Project\\Environment\\env\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\acer/nltk_data'\n    - 'c:\\\\Project\\\\Environment\\\\env\\\\nltk_data'\n    - 'c:\\\\Project\\\\Environment\\\\env\\\\share\\\\nltk_data'\n    - 'c:\\\\Project\\\\Environment\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\acer\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m     title \u001b[38;5;241m=\u001b[39m li_item\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m     15\u001b[0m     categories \u001b[38;5;241m=\u001b[39m [concept\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m concept \u001b[38;5;129;01min\u001b[39;00m li_item\u001b[38;5;241m.\u001b[39mfindAll(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, class_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcept\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m---> 16\u001b[0m     imp \u001b[38;5;241m=\u001b[39m \u001b[43mlemmatize_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([author[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m author \u001b[38;5;129;01min\u001b[39;00m authors]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     18\u001b[0m             lemmatize_stmt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(categories)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     19\u001b[0m         lemmatize_stmt(published_date)\n\u001b[0;32m     20\u001b[0m     research_output\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthors\u001b[39m\u001b[38;5;124m\"\u001b[39m:authors, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublished_date\u001b[39m\u001b[38;5;124m\"\u001b[39m:published_date, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m:title, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresearch_link\u001b[39m\u001b[38;5;124m\"\u001b[39m: research_link, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m: categories, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimp\u001b[39m\u001b[38;5;124m\"\u001b[39m: imp})\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal Document Scrapped:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(research_output))\n",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m, in \u001b[0;36mlemmatize_stmt\u001b[1;34m(statement)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize_stmt\u001b[39m(statement):    \n\u001b[1;32m---> 19\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     lemmatize_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m filtered_tokens:\n",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m, in \u001b[0;36mtokenizer\u001b[1;34m(statement)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenizer\u001b[39m(statement):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Filter out stop words & special characters\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      4\u001b[0m     special_characters \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\u001b[38;5;124m!()-â€”[]\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m;:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m, <>./?@#$\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m^&*_~+=\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m      5\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m word_tokenize(statement)\n",
      "File \u001b[1;32mc:\\Project\\Environment\\env\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Project\\Environment\\env\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Project\\Environment\\env\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Project\\Environment\\env\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\acer/nltk_data'\n    - 'c:\\\\Project\\\\Environment\\\\env\\\\nltk_data'\n    - 'c:\\\\Project\\\\Environment\\\\env\\\\share\\\\nltk_data'\n    - 'c:\\\\Project\\\\Environment\\\\env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\acer\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = 'https://pureportal.coventry.ac.uk/'\n",
    "\n",
    "print(\"Scraping\")\n",
    "\n",
    "research_output = []\n",
    "url_path = \"https://pureportal.coventry.ac.uk/en/organisations/ihw-centre-for-health-and-life-sciences-chls/publications/\"\n",
    "response = requests.get(url_path)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "li_item_tags = soup.find_all('li', class_= 'list-result-item')\n",
    "for li_item in li_item_tags:\n",
    "    research_link = li_item.find('a')['href']\n",
    "    authors = [{'author':author.text, 'url': author['href']} for author in li_item.findAll('a', 'link person')]\n",
    "    published_date = li_item.find('span', class_='date').text\n",
    "    title = li_item.find('h3', class_='title').text\n",
    "    categories = [concept.text for concept in li_item.findAll('span', class_ = 'concept')]\n",
    "    imp = lemmatize_stmt(title)  + ' ' + \\\n",
    "        \" \".join([author['url'].split('/')[-1].replace('-', ' ') for author in authors]) + ' ' + \\\n",
    "            lemmatize_stmt(\" \".join(categories)) + ' ' + \\\n",
    "        lemmatize_stmt(published_date)\n",
    "    research_output.append({\"authors\":authors, \"published_date\":published_date, \"title\":title, \"research_link\": research_link, \"categories\": categories, \"imp\": imp})\n",
    "print('\\nTotal Document Scrapped:', len(research_output))\n",
    "# Dump scraped data to json file\n",
    "with open('./scraped_data/rcih_research_output.json', 'w') as f:\n",
    "    json.dump(research_output, f, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Inverted Index Construction](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "with open('./scraped_data/rcih_research_output.json') as f:\n",
    "    rcih_research = json.loads(f.read())\n",
    "\n",
    "for doc_id, doc in enumerate(rcih_research):\n",
    "    imp = lemmatize_stmt(doc['title']  + ' ' + \" \".join(doc['categories']) + ' ' + doc['published_date']) + ' ' + \\\n",
    "            \" \".join([author['url'].split('/')[-1].replace('-', ' ') for author in doc['authors']])\n",
    "    for term in imp.split():\n",
    "        if term not in inverted_index:\n",
    "            inverted_index[term] = set()\n",
    "        inverted_index[term].add(doc_id)\n",
    "\n",
    "# Converting Set to List\n",
    "for key in inverted_index:\n",
    "    inverted_index[key] = list(inverted_index[key])\n",
    "\n",
    "with open('./scraped_data/inverted_index.json', 'w') as f:\n",
    "    json.dump(inverted_index, f, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Text Classifier](#toc0_)\n",
    "Identify whether the input scientific document is from the listed cases: \n",
    "- Health\n",
    "- Business\n",
    "- Sport\n",
    "\n",
    "Training Data link : https://www.kaggle.com/datasets/rmisra/news-category-dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[Get Training Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_json('News_Category_Dataset_v3.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_news_df = news_df[news_df['category'].isin(['BUSINESS', \"SPORTS\", \"HEALTHY LIVING\"])]\n",
    "training_news_df.loc[training_news_df['category']== \"HEALTHY LIVING\" , 'category'] = \"HEALTH\"\n",
    "training_news_df = training_news_df[['short_description', 'category']]\n",
    "training_news_df['short_description'] = training_news_df['short_description'].apply(lemmatize_stmt)\n",
    "training_news_df.to_pickle(\"training_news_df.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[MultinomialNB classifier](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['SPORTS']\n"
     ]
    }
   ],
   "source": [
    "training_news_df = pd.read_pickle('training_news_df.pkl')\n",
    "\n",
    "# Sample training data\n",
    "texts = training_news_df['short_description'].tolist()\n",
    "labels = training_news_df['category'].tolist()\n",
    "# Create feature vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "test_texts = [\"\"\"\n",
    "They fell short in what was virtually a must-win game, with the added significance of being on free-to-air television and with the opportunity to make their mark on the eager crowd that roared and applauded every England run and wicket with vigour.\n",
    "\n",
    "And they felt the disappointment, too - but there was a bigger picture.\n",
    "\n",
    "Ten years ago to the day, Nat Sciver-Brunt made her England debut against Pakistan at Louth, where spectators were few and far between and most likely consisted mostly of the players' families.\n",
    "\n",
    "But for England's debutant at Edgbaston, all-rounder Danielle Gibson, the experience could not be more of a contrast - and in the most brilliant, inspiring way.\n",
    "\n",
    "Gibson was clapped in to bowl like an Olympic long jumper at the start of their mark, each dot ball cheered like a wicket and every run saved greeted with raucous appreciation.\n",
    "\n",
    "There was diversity in numbers, too - children danced for the camera, groups of friends dressed up as Super Mario and lifeguards, boys donning England shirts with 'Knight' and 'Sciver-Brunt' on the back.\n",
    "\n",
    "As England fought back in the closing overs with three late wickets to ignite hopes of a shock victory, the crowd savoured every emotion with them.\n",
    "\"\"\"]\n",
    "test_X = vectorizer.transform(test_texts)\n",
    "test_predictions = classifier.predict(test_X)\n",
    "print('Predictions:', test_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a>[Metric Report](#toc0_)\n",
    "\n",
    "Evaluate the performance of classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.802603800140746\n",
      "Testing Accuracy: 0.7261469180973825\n",
      "F1 Score: 0.7249536376495858\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BUSINESS       0.80      0.66      0.72      1197\n",
      "      HEALTH       0.64      0.86      0.74      1350\n",
      "      SPORTS       0.84      0.62      0.71      1006\n",
      "\n",
      "    accuracy                           0.73      3553\n",
      "   macro avg       0.76      0.71      0.72      3553\n",
      "weighted avg       0.75      0.73      0.72      3553\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report\n",
    "\n",
    "# Test the classifier on training data\n",
    "train_predictions = classifier.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "print('Training Accuracy:', train_accuracy)\n",
    "\n",
    "# Test the classifier on testing data\n",
    "test_predictions = classifier.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print('Testing Accuracy:', test_accuracy)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, test_predictions, average='weighted')\n",
    "print('F1 Score:', f1)\n",
    "\n",
    "# Print classification report\n",
    "class_report = classification_report(y_test, test_predictions)\n",
    "print('\\nClassification Report:')\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
